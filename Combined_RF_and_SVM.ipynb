{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM and Random Forest Classifier\n",
    "\n",
    "Here we will train two models, a Random Forest Model and a Support Vector Machine.\n",
    "We will then combine and average their outcomes. \n",
    "Combining these two models will hopefully work to give us a much better accuracy for predicting fake news. Both models work in different ways;\n",
    "\n",
    "- Decision Trees in a random forest are all trained independently on subsets of the data. Each tree looks at random parts of the text and decides wether it thinks the news is real or fake, then a final decision is reached by whatever the majority of trees decided.\n",
    "    - RFs are less prone to outlier influence as its result depends on the outcomes of multiple trees\n",
    "    - The randomness of feature selection, and of the data each tree trains on, helps ensure that the model will give more accurate predictions for unseen data \n",
    "\n",
    "- The Support Vector Machine will try and create the best decision boundary it can to separate real and fake news. If all news articles were plotted as dots on a graph, the SVM would try and fine the best line it can that separates the real ones from the fake ones, where the line is as far from any articles as it can be.\n",
    "    - SVMs work well for classifying complex data, as data can be mapped in a higher dimension, making it easier to find a decision boundary (We can choose what kernel to use for the model, which decides how to form the decision boundary)\n",
    "    - Also generalizes well to unseen data \n",
    "\n",
    "If we combine both methods we can combine the strengths of both models and potentially get more accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Import Vectorizers\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "\n",
    "#TextPreprocess.py provides functions for preprocessing a piece of text and a dataset\n",
    "from TextPreprocess import preprocessText, preprocessDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0        8476                       You Can Smell Hillary’s Fear   \n",
       "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4         875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "4  It's primary day in New York and front-runners...  REAL  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading dataset from .csv file\n",
    "dataset = pd.read_csv('Datasets/fake_or_real_news.csv')\n",
    "\n",
    "# Separating labels from rest of dataset\n",
    "labels = dataset.label\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing data\n",
    "\n",
    "#preprocessDataset((dataset to process), (name of column with text to be processed))\n",
    "preprocessedData = preprocessDataset(dataset, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    daniel greenfield shillman journalism fellow f...\n",
       "1    google pinterest digg linkedin reddit stumbleu...\n",
       "2    u secretary state john f kerry said monday sto...\n",
       "3    kaydee king kaydeeking november 9 2016 lesson ...\n",
       "4    primary day new york frontrunners hillary clin...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessedData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing\n",
    "x_train, x_test, y_train, y_test = train_test_split(preprocessedData, labels, test_size= 0.2, random_state= 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing\n",
    "\n",
    "We need to convert the raw text we want to train our model on into numbers that our model can understand.\n",
    "\n",
    "#### <u>TF-IDF:</u>\n",
    "(Term Frequency Inverse Document Frequency)\n",
    "- How often words are found in a document and how unique they are to the document\n",
    "- Contains information on the most and least important words in the document\n",
    "- Purely based off occurrence of words, does not take into account positions of words in the text\n",
    "\n",
    "####  <u>Bag of Words:</u>\n",
    "(Count vectorizer)\n",
    "- Counts occurrences of words in the document\n",
    "- Similar to TF-IDF except doesnt look at importance of words\n",
    "\n",
    "#### <u>Hashing Vectorizer:</u>\n",
    "- Convert a document to a matrix of token occurrences.\n",
    "- Uses hashing to map token strings to feature indexes\n",
    "\n",
    "#### <u>N-Grams:</u>\n",
    "- Combinations of all adjacent words of length N\n",
    "- Counts the occurrences of these N-grams in the document\n",
    "- Maintains word order, but there is a tradeoff with choosing a value for N. Smaller N may result in less useful information, a higher N will give however give a very large matrix with lots of features. May be too many features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFIDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words= 'english', max_df= 0.7)\n",
    "\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(x_train)\n",
    "tfidf_test = tfidf_vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of Words Vectorizer\n",
    "bow_vectorizer = CountVectorizer(stop_words= 'english')\n",
    "\n",
    "bow_train = bow_vectorizer.fit_transform(x_train)\n",
    "bow_test = bow_vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hashing Vectorizer\n",
    "hashing_vectorizer = HashingVectorizer(n_features= 2**20)\n",
    "\n",
    "hashed_train = hashing_vectorizer.fit_transform(x_train)\n",
    "hashed_test = hashing_vectorizer.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N-Grams\n",
    "#Used for tokenizing then using count vectorizer to turn the ngrams to numbers\n",
    "\n",
    "ngram_range = (1,3)\n",
    "\n",
    "ngramVectorizer = CountVectorizer(ngram_range=ngram_range)\n",
    "\n",
    "ngram_train = ngramVectorizer.fit_transform(x_train)\n",
    "ngram_test = ngramVectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N-Grams TF-IDF\n",
    "\n",
    "ngram_range = (1,3)\n",
    "\n",
    "ngram_TFIDF_Vec = TfidfVectorizer(ngram_range=ngram_range)\n",
    "\n",
    "ngram_tfidf_train = ngram_TFIDF_Vec.fit_transform(x_train)\n",
    "ngram_tfidf_test = ngram_TFIDF_Vec.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the SVM\n",
    "# Probability set to true so we can use soft voting with the random forest\n",
    "svm_model = svm.SVC(kernel='linear', probability=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the random forest model\n",
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Both models using voting\n",
    "Allows us to get the majority output of both models and combine them into one classifier.\n",
    "\n",
    "Soft voting predicts the label based on predicted probabilities.\n",
    "\n",
    "**Takes a long time to train (2 - 3 minutes generally, up to at least 11 with N-grams)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combined_Models = VotingClassifier(estimators=[('svm', svm_model), ('randomForest', rf)], voting='soft')\n",
    "\n",
    "# Fit our training data to the ensemble model\n",
    "\n",
    "Combined_Models = Combined_Models.fit(tfidf_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction and Accuracy\n",
    "\n",
    "Some sample accuracy scores (** There are other metrics for accuracy we should look at too **)\n",
    "- Using TF_IDF = 0.934\n",
    "- Using BOW = 0.89\n",
    "- Using hashing = 0.927\n",
    "- Using N-grams (N = 3) and BOW = 0.887 [more confidence in predictions than without n-grams]\n",
    "- Using N-grams (N = 3) and TF-IDF = 0.922 [more confidence in predictions than without n-grams]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict labels for testing set \n",
    "y_pred_combined = Combined_Models.predict(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Accuracy:  0.9337016574585635\n"
     ]
    }
   ],
   "source": [
    "#accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "#print(\"SVM Accuracy: \", accuracy_svm)\n",
    "#accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "#print(\"Random Forest Accuracy: \", accuracy_rf)\n",
    "\n",
    "accuracy_combined = accuracy_score(y_test, y_pred_combined)\n",
    "print(\"Combined Accuracy: \", accuracy_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test own Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label Combined Models:  ['FAKE']\n",
      "Model Confidence that article is Fake =  0.6393507372049535\n",
      "Model Confidence that article is Real =  0.3606492627950466\n"
     ]
    }
   ],
   "source": [
    "# Sample text taken from RTE.ie\n",
    "filePath = 'Test_Text.txt'\n",
    "\n",
    "with open(filePath, 'r', encoding= 'utf-8') as file:\n",
    "    sample_text = file.read()\n",
    "\n",
    "#preprocessText(Text to be processed)\n",
    "preprocessedText = preprocessText(sample_text)\n",
    "\n",
    "tfidf_sampleText = tfidf_vectorizer.transform([preprocessedText])\n",
    "\n",
    "samplePred_Combined = Combined_Models.predict(tfidf_sampleText)\n",
    "\n",
    "Combined_confidence = Combined_Models.predict_proba(tfidf_sampleText)\n",
    "\n",
    "print(\"Predicted Label Combined Models: \", samplePred_Combined)\n",
    "print(\"Model Confidence that article is Fake = \", Combined_confidence[0][0])\n",
    "print(\"Model Confidence that article is Real = \", Combined_confidence[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
